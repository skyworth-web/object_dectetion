{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from ultralytics import YOLO\n",
    "import ipywidgets as widgets\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: Load YOLOv8 model\n",
    "model = YOLO('yolov8m.pt')  # fast + lightweight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1898a7783d47f7a12dee7ece74a50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Select classes to detect:', index=(0, 15, 16), layout=Layout(width='50%'), options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get COCO class names\n",
    "COCO_CLASSES = model.names\n",
    "\n",
    "# Cell 4: User selects classes\n",
    "class_selector = widgets.SelectMultiple(\n",
    "    options=list(COCO_CLASSES.values()),\n",
    "    value=['person', 'cat', 'dog'],\n",
    "    description='Select classes to detect:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "display(class_selector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf3dd3b74e64a5095dca69e54a1ac6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='dog_and_cat.mp4', description='Video File Path:', layout=Layout(width='75%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Cell 5: File selector widget\n",
    "video_path_widget = widgets.Text(\n",
    "    value='dog_and_cat.mp4',\n",
    "    description='Video File Path:',\n",
    "    layout=widgets.Layout(width='75%')\n",
    ")\n",
    "display(video_path_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Helper function to compute relative position\n",
    "def get_relative_position(box_center, frame_center):\n",
    "    x_rel = 2 * (box_center[0] - frame_center[0]) / frame_center[0]\n",
    "    y_rel = 2 * (box_center[1] - frame_center[1]) / frame_center[1]\n",
    "    return round(x_rel, 2), round(y_rel, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 Processing video...\n",
      "\n",
      "0: 384x640 1 dog, 252.7ms\n",
      "Speed: 7.2ms preprocess, 252.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 0, 'id': 0, 'class': 'dog', 'confidence': 0.82, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.47)}}\n",
      "\n",
      "0: 384x640 1 dog, 255.8ms\n",
      "Speed: 2.2ms preprocess, 255.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 1, 'id': 0, 'class': 'dog', 'confidence': 0.82, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.47)}}\n",
      "\n",
      "0: 384x640 1 bus, 1 dog, 247.3ms\n",
      "Speed: 2.8ms preprocess, 247.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 2, 'id': 0, 'class': 'dog', 'confidence': 0.72, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.43)}}\n",
      "\n",
      "0: 384x640 1 bus, 1 dog, 220.5ms\n",
      "Speed: 1.3ms preprocess, 220.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 3, 'id': 0, 'class': 'dog', 'confidence': 0.66, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.43)}}\n",
      "\n",
      "0: 384x640 1 bus, 2 dogs, 224.1ms\n",
      "Speed: 2.0ms preprocess, 224.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 4, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(0.86), 'y': np.float64(0.33)}}\n",
      "{'frame': 4, 'id': 2, 'class': 'dog', 'confidence': 0.52, 'rel_position': {'x': np.float64(-1.56), 'y': np.float64(-0.07)}}\n",
      "\n",
      "0: 384x640 1 bus, 1 cat, 2 dogs, 210.9ms\n",
      "Speed: 1.7ms preprocess, 210.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 5, 'id': 0, 'class': 'dog', 'confidence': 0.76, 'rel_position': {'x': np.float64(-1.58), 'y': np.float64(0.18)}}\n",
      "{'frame': 5, 'id': 1, 'class': 'dog', 'confidence': 0.67, 'rel_position': {'x': np.float64(0.87), 'y': np.float64(0.4)}}\n",
      "{'frame': 5, 'id': 2, 'class': 'cat', 'confidence': 0.49, 'rel_position': {'x': np.float64(0.87), 'y': np.float64(0.41)}}\n",
      "\n",
      "0: 384x640 1 bus, 1 cat, 2 dogs, 215.7ms\n",
      "Speed: 1.8ms preprocess, 215.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 6, 'id': 0, 'class': 'dog', 'confidence': 0.77, 'rel_position': {'x': np.float64(-1.58), 'y': np.float64(0.18)}}\n",
      "{'frame': 6, 'id': 1, 'class': 'dog', 'confidence': 0.66, 'rel_position': {'x': np.float64(0.87), 'y': np.float64(0.4)}}\n",
      "{'frame': 6, 'id': 2, 'class': 'cat', 'confidence': 0.47, 'rel_position': {'x': np.float64(0.87), 'y': np.float64(0.41)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 214.2ms\n",
      "Speed: 1.9ms preprocess, 214.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 7, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.89), 'y': np.float64(0.24)}}\n",
      "{'frame': 7, 'id': 1, 'class': 'cat', 'confidence': 0.55, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.31)}}\n",
      "{'frame': 7, 'id': 2, 'class': 'dog', 'confidence': 0.51, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.31)}}\n",
      "\n",
      "0: 384x640 2 dogs, 222.0ms\n",
      "Speed: 2.5ms preprocess, 222.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 8, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.88), 'y': np.float64(0.17)}}\n",
      "{'frame': 8, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.28)}}\n",
      "\n",
      "0: 384x640 2 dogs, 236.3ms\n",
      "Speed: 2.7ms preprocess, 236.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 9, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.88), 'y': np.float64(0.18)}}\n",
      "{'frame': 9, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.28)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 219.3ms\n",
      "Speed: 3.2ms preprocess, 219.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 10, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(-0.85), 'y': np.float64(0.11)}}\n",
      "{'frame': 10, 'id': 1, 'class': 'cat', 'confidence': 0.89, 'rel_position': {'x': np.float64(0.94), 'y': np.float64(0.25)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 220.6ms\n",
      "Speed: 1.1ms preprocess, 220.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 11, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(-0.83), 'y': np.float64(0.08)}}\n",
      "{'frame': 11, 'id': 1, 'class': 'cat', 'confidence': 0.8, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 223.5ms\n",
      "Speed: 2.2ms preprocess, 223.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 12, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(-0.83), 'y': np.float64(0.08)}}\n",
      "{'frame': 12, 'id': 1, 'class': 'cat', 'confidence': 0.8, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 230.6ms\n",
      "Speed: 3.1ms preprocess, 230.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 13, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.8), 'y': np.float64(0.06)}}\n",
      "{'frame': 13, 'id': 1, 'class': 'dog', 'confidence': 0.51, 'rel_position': {'x': np.float64(0.99), 'y': np.float64(0.2)}}\n",
      "{'frame': 13, 'id': 2, 'class': 'cat', 'confidence': 0.5, 'rel_position': {'x': np.float64(0.99), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 225.0ms\n",
      "Speed: 2.6ms preprocess, 225.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 14, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.8), 'y': np.float64(0.06)}}\n",
      "{'frame': 14, 'id': 1, 'class': 'cat', 'confidence': 0.52, 'rel_position': {'x': np.float64(0.99), 'y': np.float64(0.2)}}\n",
      "{'frame': 14, 'id': 2, 'class': 'dog', 'confidence': 0.49, 'rel_position': {'x': np.float64(0.99), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 2 dogs, 214.9ms\n",
      "Speed: 1.3ms preprocess, 214.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 15, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.78), 'y': np.float64(0.03)}}\n",
      "{'frame': 15, 'id': 1, 'class': 'dog', 'confidence': 0.77, 'rel_position': {'x': np.float64(1.03), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 211.6ms\n",
      "Speed: 1.7ms preprocess, 211.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 16, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.76), 'y': np.float64(0.03)}}\n",
      "{'frame': 16, 'id': 1, 'class': 'dog', 'confidence': 0.7, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(0.18)}}\n",
      "{'frame': 16, 'id': 2, 'class': 'cat', 'confidence': 0.45, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(0.19)}}\n",
      "\n",
      "0: 384x640 2 dogs, 211.2ms\n",
      "Speed: 1.6ms preprocess, 211.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 17, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.76), 'y': np.float64(0.03)}}\n",
      "{'frame': 17, 'id': 1, 'class': 'dog', 'confidence': 0.73, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(0.18)}}\n",
      "\n",
      "0: 384x640 2 dogs, 216.1ms\n",
      "Speed: 1.9ms preprocess, 216.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 18, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.73), 'y': np.float64(0.07)}}\n",
      "{'frame': 18, 'id': 1, 'class': 'dog', 'confidence': 0.8, 'rel_position': {'x': np.float64(1.08), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 2 dogs, 228.4ms\n",
      "Speed: 1.3ms preprocess, 228.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 19, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.73), 'y': np.float64(0.07)}}\n",
      "{'frame': 19, 'id': 1, 'class': 'dog', 'confidence': 0.8, 'rel_position': {'x': np.float64(1.08), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 2 dogs, 281.4ms\n",
      "Speed: 2.1ms preprocess, 281.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 20, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.7), 'y': np.float64(0.06)}}\n",
      "{'frame': 20, 'id': 1, 'class': 'dog', 'confidence': 0.75, 'rel_position': {'x': np.float64(1.09), 'y': np.float64(0.09)}}\n",
      "\n",
      "0: 384x640 2 dogs, 266.9ms\n",
      "Speed: 1.8ms preprocess, 266.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 21, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.68), 'y': np.float64(0.07)}}\n",
      "{'frame': 21, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.11), 'y': np.float64(0.07)}}\n",
      "\n",
      "0: 384x640 2 dogs, 236.5ms\n",
      "Speed: 3.2ms preprocess, 236.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 22, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.68), 'y': np.float64(0.07)}}\n",
      "{'frame': 22, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.11), 'y': np.float64(0.07)}}\n",
      "\n",
      "0: 384x640 2 dogs, 222.9ms\n",
      "Speed: 1.8ms preprocess, 222.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 23, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(-0.64), 'y': np.float64(0.09)}}\n",
      "{'frame': 23, 'id': 1, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(1.22), 'y': np.float64(0.09)}}\n",
      "\n",
      "0: 384x640 2 dogs, 261.3ms\n",
      "Speed: 2.5ms preprocess, 261.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 24, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(-0.64), 'y': np.float64(0.09)}}\n",
      "{'frame': 24, 'id': 1, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(1.22), 'y': np.float64(0.09)}}\n",
      "\n",
      "0: 384x640 2 dogs, 268.5ms\n",
      "Speed: 2.2ms preprocess, 268.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 25, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.6), 'y': np.float64(0.1)}}\n",
      "{'frame': 25, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.2), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 2 dogs, 208.8ms\n",
      "Speed: 1.7ms preprocess, 208.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 26, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.52), 'y': np.float64(0.11)}}\n",
      "{'frame': 26, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 2 dogs, 231.0ms\n",
      "Speed: 1.3ms preprocess, 231.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 27, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.52), 'y': np.float64(0.11)}}\n",
      "{'frame': 27, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 3 dogs, 274.4ms\n",
      "Speed: 2.0ms preprocess, 274.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 28, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.28)}}\n",
      "{'frame': 28, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(-0.51), 'y': np.float64(0.11)}}\n",
      "{'frame': 28, 'id': 2, 'class': 'dog', 'confidence': 0.43, 'rel_position': {'x': np.float64(-0.2), 'y': np.float64(-0.92)}}\n",
      "\n",
      "0: 384x640 3 dogs, 235.1ms\n",
      "Speed: 1.5ms preprocess, 235.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 29, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.28)}}\n",
      "{'frame': 29, 'id': 1, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(-0.51), 'y': np.float64(0.11)}}\n",
      "{'frame': 29, 'id': 2, 'class': 'dog', 'confidence': 0.45, 'rel_position': {'x': np.float64(-0.2), 'y': np.float64(-0.91)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 203.3ms\n",
      "Speed: 1.9ms preprocess, 203.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 30, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(-0.38), 'y': np.float64(0.09)}}\n",
      "{'frame': 30, 'id': 1, 'class': 'dog', 'confidence': 0.68, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.32)}}\n",
      "{'frame': 30, 'id': 2, 'class': 'cat', 'confidence': 0.59, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.32)}}\n",
      "\n",
      "0: 384x640 1 cat, 3 dogs, 208.9ms\n",
      "Speed: 1.8ms preprocess, 208.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 31, 'id': 0, 'class': 'dog', 'confidence': 0.74, 'rel_position': {'x': np.float64(-0.34), 'y': np.float64(0.07)}}\n",
      "{'frame': 31, 'id': 1, 'class': 'cat', 'confidence': 0.71, 'rel_position': {'x': np.float64(1.18), 'y': np.float64(0.32)}}\n",
      "{'frame': 31, 'id': 2, 'class': 'dog', 'confidence': 0.51, 'rel_position': {'x': np.float64(1.18), 'y': np.float64(0.32)}}\n",
      "{'frame': 31, 'id': 3, 'class': 'dog', 'confidence': 0.47, 'rel_position': {'x': np.float64(-0.48), 'y': np.float64(0.23)}}\n",
      "\n",
      "0: 384x640 1 cat, 3 dogs, 218.6ms\n",
      "Speed: 1.2ms preprocess, 218.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 32, 'id': 0, 'class': 'dog', 'confidence': 0.78, 'rel_position': {'x': np.float64(-0.34), 'y': np.float64(0.07)}}\n",
      "{'frame': 32, 'id': 1, 'class': 'cat', 'confidence': 0.71, 'rel_position': {'x': np.float64(1.18), 'y': np.float64(0.32)}}\n",
      "{'frame': 32, 'id': 2, 'class': 'dog', 'confidence': 0.54, 'rel_position': {'x': np.float64(1.18), 'y': np.float64(0.32)}}\n",
      "{'frame': 32, 'id': 3, 'class': 'dog', 'confidence': 0.44, 'rel_position': {'x': np.float64(-0.49), 'y': np.float64(0.23)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 213.9ms\n",
      "Speed: 2.4ms preprocess, 213.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 33, 'id': 0, 'class': 'cat', 'confidence': 0.81, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.32)}}\n",
      "{'frame': 33, 'id': 1, 'class': 'dog', 'confidence': 0.79, 'rel_position': {'x': np.float64(-0.3), 'y': np.float64(0.02)}}\n",
      "{'frame': 33, 'id': 2, 'class': 'dog', 'confidence': 0.48, 'rel_position': {'x': np.float64(-0.45), 'y': np.float64(0.24)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 221.6ms\n",
      "Speed: 1.3ms preprocess, 221.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 34, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.3), 'y': np.float64(0.03)}}\n",
      "{'frame': 34, 'id': 1, 'class': 'cat', 'confidence': 0.83, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.32)}}\n",
      "\n",
      "0: 384x640 1 car, 2 dogs, 216.5ms\n",
      "Speed: 1.1ms preprocess, 216.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 35, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.33), 'y': np.float64(-0.05)}}\n",
      "{'frame': 35, 'id': 1, 'class': 'dog', 'confidence': 0.85, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.3)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 2 frisbees, 218.0ms\n",
      "Speed: 2.2ms preprocess, 218.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 36, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(-0.33), 'y': np.float64(-0.09)}}\n",
      "{'frame': 36, 'id': 1, 'class': 'dog', 'confidence': 0.79, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.32)}}\n",
      "{'frame': 36, 'id': 2, 'class': 'cat', 'confidence': 0.49, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.31)}}\n",
      "\n",
      "0: 384x640 1 car, 1 cat, 2 dogs, 2 frisbees, 220.0ms\n",
      "Speed: 1.2ms preprocess, 220.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 37, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(-0.33), 'y': np.float64(-0.09)}}\n",
      "{'frame': 37, 'id': 1, 'class': 'dog', 'confidence': 0.79, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.32)}}\n",
      "{'frame': 37, 'id': 2, 'class': 'cat', 'confidence': 0.49, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.31)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 232.7ms\n",
      "Speed: 1.9ms preprocess, 232.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 38, 'id': 0, 'class': 'cat', 'confidence': 0.88, 'rel_position': {'x': np.float64(1.17), 'y': np.float64(0.3)}}\n",
      "{'frame': 38, 'id': 1, 'class': 'dog', 'confidence': 0.87, 'rel_position': {'x': np.float64(-0.35), 'y': np.float64(-0.04)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 218.7ms\n",
      "Speed: 1.3ms preprocess, 218.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 39, 'id': 0, 'class': 'dog', 'confidence': 0.85, 'rel_position': {'x': np.float64(-0.35), 'y': np.float64(0.05)}}\n",
      "{'frame': 39, 'id': 1, 'class': 'dog', 'confidence': 0.61, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.29)}}\n",
      "{'frame': 39, 'id': 2, 'class': 'cat', 'confidence': 0.6, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.29)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 207.4ms\n",
      "Speed: 1.7ms preprocess, 207.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 40, 'id': 0, 'class': 'dog', 'confidence': 0.86, 'rel_position': {'x': np.float64(-0.34), 'y': np.float64(0.04)}}\n",
      "{'frame': 40, 'id': 1, 'class': 'cat', 'confidence': 0.7, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.29)}}\n",
      "{'frame': 40, 'id': 2, 'class': 'dog', 'confidence': 0.58, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.29)}}\n",
      "\n",
      "0: 384x640 2 dogs, 224.2ms\n",
      "Speed: 1.2ms preprocess, 224.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 41, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(1.14), 'y': np.float64(0.26)}}\n",
      "{'frame': 41, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(-0.36), 'y': np.float64(0.06)}}\n",
      "\n",
      "0: 384x640 2 dogs, 216.9ms\n",
      "Speed: 1.7ms preprocess, 216.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 42, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(1.14), 'y': np.float64(0.26)}}\n",
      "{'frame': 42, 'id': 1, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.36), 'y': np.float64(0.06)}}\n",
      "\n",
      "0: 384x640 2 dogs, 211.8ms\n",
      "Speed: 1.3ms preprocess, 211.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 43, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(0.23)}}\n",
      "{'frame': 43, 'id': 1, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(-0.42), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 2 dogs, 214.5ms\n",
      "Speed: 1.8ms preprocess, 214.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 44, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(0.23)}}\n",
      "{'frame': 44, 'id': 1, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(-0.42), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 2 dogs, 218.0ms\n",
      "Speed: 1.3ms preprocess, 218.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 45, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(0.21)}}\n",
      "{'frame': 45, 'id': 1, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(-0.47), 'y': np.float64(0.24)}}\n",
      "\n",
      "0: 384x640 2 dogs, 216.8ms\n",
      "Speed: 2.5ms preprocess, 216.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 46, 'id': 0, 'class': 'dog', 'confidence': 0.97, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(0.13)}}\n",
      "{'frame': 46, 'id': 1, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.62), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 2 dogs, 242.0ms\n",
      "Speed: 1.7ms preprocess, 242.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 47, 'id': 0, 'class': 'dog', 'confidence': 0.97, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(0.13)}}\n",
      "{'frame': 47, 'id': 1, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(-0.62), 'y': np.float64(0.19)}}\n",
      "\n",
      "0: 384x640 2 dogs, 230.6ms\n",
      "Speed: 2.4ms preprocess, 230.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 48, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(1.12), 'y': np.float64(0.12)}}\n",
      "{'frame': 48, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(-0.69), 'y': np.float64(0.16)}}\n",
      "\n",
      "0: 384x640 2 dogs, 229.0ms\n",
      "Speed: 2.4ms preprocess, 229.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 49, 'id': 0, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.12), 'y': np.float64(0.12)}}\n",
      "{'frame': 49, 'id': 1, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.69), 'y': np.float64(0.16)}}\n",
      "\n",
      "0: 384x640 2 dogs, 229.2ms\n",
      "Speed: 2.8ms preprocess, 229.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 50, 'id': 0, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(-0.81), 'y': np.float64(0.14)}}\n",
      "{'frame': 50, 'id': 1, 'class': 'dog', 'confidence': 0.79, 'rel_position': {'x': np.float64(1.08), 'y': np.float64(0.11)}}\n",
      "\n",
      "0: 384x640 1 cat, 3 dogs, 213.1ms\n",
      "Speed: 1.9ms preprocess, 213.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 51, 'id': 0, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(-0.86), 'y': np.float64(0.18)}}\n",
      "{'frame': 51, 'id': 1, 'class': 'cat', 'confidence': 0.68, 'rel_position': {'x': np.float64(1.06), 'y': np.float64(0.07)}}\n",
      "{'frame': 51, 'id': 2, 'class': 'dog', 'confidence': 0.48, 'rel_position': {'x': np.float64(1.06), 'y': np.float64(0.06)}}\n",
      "{'frame': 51, 'id': 3, 'class': 'dog', 'confidence': 0.4, 'rel_position': {'x': np.float64(1.82), 'y': np.float64(-0.32)}}\n",
      "\n",
      "0: 384x640 1 cat, 3 dogs, 232.5ms\n",
      "Speed: 1.6ms preprocess, 232.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 52, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(-0.86), 'y': np.float64(0.19)}}\n",
      "{'frame': 52, 'id': 1, 'class': 'cat', 'confidence': 0.72, 'rel_position': {'x': np.float64(1.06), 'y': np.float64(0.07)}}\n",
      "{'frame': 52, 'id': 2, 'class': 'dog', 'confidence': 0.46, 'rel_position': {'x': np.float64(1.06), 'y': np.float64(0.06)}}\n",
      "{'frame': 52, 'id': 3, 'class': 'dog', 'confidence': 0.41, 'rel_position': {'x': np.float64(1.82), 'y': np.float64(-0.32)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 206.7ms\n",
      "Speed: 1.2ms preprocess, 206.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 53, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.85), 'y': np.float64(0.18)}}\n",
      "{'frame': 53, 'id': 1, 'class': 'cat', 'confidence': 0.73, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.01)}}\n",
      "{'frame': 53, 'id': 2, 'class': 'dog', 'confidence': 0.61, 'rel_position': {'x': np.float64(1.04), 'y': np.float64(-0.03)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 198.7ms\n",
      "Speed: 2.2ms preprocess, 198.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 54, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.85), 'y': np.float64(0.18)}}\n",
      "{'frame': 54, 'id': 1, 'class': 'cat', 'confidence': 0.76, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.02)}}\n",
      "{'frame': 54, 'id': 2, 'class': 'dog', 'confidence': 0.55, 'rel_position': {'x': np.float64(1.04), 'y': np.float64(-0.03)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 237.7ms\n",
      "Speed: 2.9ms preprocess, 237.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 55, 'id': 0, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(-0.86), 'y': np.float64(0.04)}}\n",
      "{'frame': 55, 'id': 1, 'class': 'dog', 'confidence': 0.76, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.07)}}\n",
      "{'frame': 55, 'id': 2, 'class': 'cat', 'confidence': 0.54, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.08)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 271.6ms\n",
      "Speed: 3.5ms preprocess, 271.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 56, 'id': 0, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(-0.85), 'y': np.float64(0.16)}}\n",
      "{'frame': 56, 'id': 1, 'class': 'dog', 'confidence': 0.7, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.1)}}\n",
      "{'frame': 56, 'id': 2, 'class': 'cat', 'confidence': 0.56, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.1)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 219.7ms\n",
      "Speed: 2.7ms preprocess, 219.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 57, 'id': 0, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(-0.85), 'y': np.float64(-0.03)}}\n",
      "{'frame': 57, 'id': 1, 'class': 'dog', 'confidence': 0.73, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.1)}}\n",
      "{'frame': 57, 'id': 2, 'class': 'cat', 'confidence': 0.53, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(-0.1)}}\n",
      "\n",
      "0: 384x640 2 dogs, 227.9ms\n",
      "Speed: 3.2ms preprocess, 227.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 58, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.83), 'y': np.float64(-0.09)}}\n",
      "{'frame': 58, 'id': 1, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.03), 'y': np.float64(-0.07)}}\n",
      "\n",
      "0: 384x640 2 dogs, 213.6ms\n",
      "Speed: 2.2ms preprocess, 213.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 59, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.83), 'y': np.float64(-0.1)}}\n",
      "{'frame': 59, 'id': 1, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.03), 'y': np.float64(-0.07)}}\n",
      "\n",
      "0: 384x640 2 dogs, 216.5ms\n",
      "Speed: 2.0ms preprocess, 216.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 60, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(-0.85), 'y': np.float64(-0.14)}}\n",
      "{'frame': 60, 'id': 1, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(1.04), 'y': np.float64(-0.03)}}\n",
      "\n",
      "0: 384x640 2 dogs, 212.5ms\n",
      "Speed: 1.9ms preprocess, 212.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 61, 'id': 0, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.04), 'y': np.float64(0.14)}}\n",
      "{'frame': 61, 'id': 1, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.84), 'y': np.float64(-0.22)}}\n",
      "\n",
      "0: 384x640 2 dogs, 218.9ms\n",
      "Speed: 1.8ms preprocess, 218.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 62, 'id': 0, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(1.04), 'y': np.float64(0.14)}}\n",
      "{'frame': 62, 'id': 1, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.84), 'y': np.float64(-0.22)}}\n",
      "\n",
      "0: 384x640 2 dogs, 223.7ms\n",
      "Speed: 1.5ms preprocess, 223.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 63, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(1.09), 'y': np.float64(0.18)}}\n",
      "{'frame': 63, 'id': 1, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.9), 'y': np.float64(-0.25)}}\n",
      "\n",
      "0: 384x640 2 dogs, 215.0ms\n",
      "Speed: 1.6ms preprocess, 215.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 64, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(1.09), 'y': np.float64(0.18)}}\n",
      "{'frame': 64, 'id': 1, 'class': 'dog', 'confidence': 0.91, 'rel_position': {'x': np.float64(-0.9), 'y': np.float64(-0.25)}}\n",
      "\n",
      "0: 384x640 2 dogs, 222.8ms\n",
      "Speed: 1.8ms preprocess, 222.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 65, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(0.17)}}\n",
      "{'frame': 65, 'id': 1, 'class': 'dog', 'confidence': 0.87, 'rel_position': {'x': np.float64(-0.89), 'y': np.float64(-0.28)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 239.0ms\n",
      "Speed: 1.8ms preprocess, 239.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 66, 'id': 0, 'class': 'dog', 'confidence': 0.77, 'rel_position': {'x': np.float64(-0.86), 'y': np.float64(-0.27)}}\n",
      "{'frame': 66, 'id': 1, 'class': 'cat', 'confidence': 0.72, 'rel_position': {'x': np.float64(1.2), 'y': np.float64(0.14)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 214.0ms\n",
      "Speed: 1.6ms preprocess, 214.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 67, 'id': 0, 'class': 'cat', 'confidence': 0.77, 'rel_position': {'x': np.float64(1.21), 'y': np.float64(0.14)}}\n",
      "{'frame': 67, 'id': 1, 'class': 'dog', 'confidence': 0.76, 'rel_position': {'x': np.float64(-0.87), 'y': np.float64(-0.27)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 218.0ms\n",
      "Speed: 1.3ms preprocess, 218.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 68, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.76), 'y': np.float64(-0.28)}}\n",
      "{'frame': 68, 'id': 1, 'class': 'dog', 'confidence': 0.65, 'rel_position': {'x': np.float64(1.21), 'y': np.float64(0.12)}}\n",
      "{'frame': 68, 'id': 2, 'class': 'cat', 'confidence': 0.48, 'rel_position': {'x': np.float64(1.21), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 2 dogs, 215.3ms\n",
      "Speed: 1.4ms preprocess, 215.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 69, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(-0.77), 'y': np.float64(-0.28)}}\n",
      "{'frame': 69, 'id': 1, 'class': 'dog', 'confidence': 0.65, 'rel_position': {'x': np.float64(1.21), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 2 dogs, 216.6ms\n",
      "Speed: 1.6ms preprocess, 216.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 70, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.87), 'y': np.float64(-0.26)}}\n",
      "{'frame': 70, 'id': 1, 'class': 'dog', 'confidence': 0.81, 'rel_position': {'x': np.float64(1.1), 'y': np.float64(0.06)}}\n",
      "\n",
      "0: 384x640 2 dogs, 208.5ms\n",
      "Speed: 1.1ms preprocess, 208.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 71, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.88), 'y': np.float64(-0.25)}}\n",
      "{'frame': 71, 'id': 1, 'class': 'dog', 'confidence': 0.79, 'rel_position': {'x': np.float64(1.08), 'y': np.float64(0.02)}}\n",
      "\n",
      "0: 384x640 2 dogs, 211.7ms\n",
      "Speed: 1.2ms preprocess, 211.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 72, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.88), 'y': np.float64(-0.25)}}\n",
      "{'frame': 72, 'id': 1, 'class': 'dog', 'confidence': 0.78, 'rel_position': {'x': np.float64(1.08), 'y': np.float64(0.02)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 215.7ms\n",
      "Speed: 1.2ms preprocess, 215.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 73, 'id': 0, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(-0.89), 'y': np.float64(-0.27)}}\n",
      "{'frame': 73, 'id': 1, 'class': 'cat', 'confidence': 0.81, 'rel_position': {'x': np.float64(1.1), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 213.3ms\n",
      "Speed: 1.3ms preprocess, 213.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 74, 'id': 0, 'class': 'dog', 'confidence': 0.86, 'rel_position': {'x': np.float64(-0.89), 'y': np.float64(-0.27)}}\n",
      "{'frame': 74, 'id': 1, 'class': 'cat', 'confidence': 0.82, 'rel_position': {'x': np.float64(1.1), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 226.6ms\n",
      "Speed: 2.6ms preprocess, 226.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 75, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(-0.9), 'y': np.float64(-0.3)}}\n",
      "{'frame': 75, 'id': 1, 'class': 'cat', 'confidence': 0.66, 'rel_position': {'x': np.float64(1.2), 'y': np.float64(0.06)}}\n",
      "{'frame': 75, 'id': 2, 'class': 'dog', 'confidence': 0.42, 'rel_position': {'x': np.float64(1.2), 'y': np.float64(0.06)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 213.8ms\n",
      "Speed: 1.4ms preprocess, 213.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 76, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(-0.91), 'y': np.float64(-0.33)}}\n",
      "{'frame': 76, 'id': 1, 'class': 'cat', 'confidence': 0.61, 'rel_position': {'x': np.float64(1.23), 'y': np.float64(0.04)}}\n",
      "{'frame': 76, 'id': 2, 'class': 'dog', 'confidence': 0.5, 'rel_position': {'x': np.float64(1.22), 'y': np.float64(0.04)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 216.6ms\n",
      "Speed: 1.3ms preprocess, 216.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 77, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(-0.91), 'y': np.float64(-0.33)}}\n",
      "{'frame': 77, 'id': 1, 'class': 'cat', 'confidence': 0.62, 'rel_position': {'x': np.float64(1.23), 'y': np.float64(0.04)}}\n",
      "{'frame': 77, 'id': 2, 'class': 'dog', 'confidence': 0.48, 'rel_position': {'x': np.float64(1.22), 'y': np.float64(0.04)}}\n",
      "\n",
      "0: 384x640 2 dogs, 248.0ms\n",
      "Speed: 2.6ms preprocess, 248.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 78, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(-0.92), 'y': np.float64(-0.27)}}\n",
      "{'frame': 78, 'id': 1, 'class': 'dog', 'confidence': 0.7, 'rel_position': {'x': np.float64(1.19), 'y': np.float64(-0.03)}}\n",
      "\n",
      "0: 384x640 2 dogs, 242.9ms\n",
      "Speed: 1.8ms preprocess, 242.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 79, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(-0.92), 'y': np.float64(-0.27)}}\n",
      "{'frame': 79, 'id': 1, 'class': 'dog', 'confidence': 0.76, 'rel_position': {'x': np.float64(1.19), 'y': np.float64(-0.03)}}\n",
      "\n",
      "0: 384x640 2 dogs, 217.7ms\n",
      "Speed: 2.4ms preprocess, 217.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 80, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(-0.93), 'y': np.float64(-0.29)}}\n",
      "{'frame': 80, 'id': 1, 'class': 'dog', 'confidence': 0.71, 'rel_position': {'x': np.float64(1.17), 'y': np.float64(-0.08)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 222.5ms\n",
      "Speed: 2.2ms preprocess, 222.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 81, 'id': 0, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(-1.02), 'y': np.float64(-0.28)}}\n",
      "{'frame': 81, 'id': 1, 'class': 'dog', 'confidence': 0.67, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.02)}}\n",
      "{'frame': 81, 'id': 2, 'class': 'cat', 'confidence': 0.56, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.03)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 217.1ms\n",
      "Speed: 1.8ms preprocess, 217.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 82, 'id': 0, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(-1.02), 'y': np.float64(-0.28)}}\n",
      "{'frame': 82, 'id': 1, 'class': 'cat', 'confidence': 0.61, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.03)}}\n",
      "{'frame': 82, 'id': 2, 'class': 'dog', 'confidence': 0.6, 'rel_position': {'x': np.float64(1.16), 'y': np.float64(0.02)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 247.4ms\n",
      "Speed: 2.4ms preprocess, 247.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 83, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(-1.07), 'y': np.float64(-0.27)}}\n",
      "{'frame': 83, 'id': 1, 'class': 'dog', 'confidence': 0.73, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(-0.02)}}\n",
      "{'frame': 83, 'id': 2, 'class': 'cat', 'confidence': 0.43, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 2 dogs, 220.6ms\n",
      "Speed: 2.0ms preprocess, 220.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 84, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(-1.07), 'y': np.float64(-0.26)}}\n",
      "{'frame': 84, 'id': 1, 'class': 'dog', 'confidence': 0.76, 'rel_position': {'x': np.float64(1.13), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 1 skis, 217.5ms\n",
      "Speed: 1.3ms preprocess, 217.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 85, 'id': 0, 'class': 'dog', 'confidence': 0.84, 'rel_position': {'x': np.float64(-1.13), 'y': np.float64(-0.26)}}\n",
      "{'frame': 85, 'id': 1, 'class': 'dog', 'confidence': 0.73, 'rel_position': {'x': np.float64(1.1), 'y': np.float64(-0.02)}}\n",
      "{'frame': 85, 'id': 2, 'class': 'cat', 'confidence': 0.42, 'rel_position': {'x': np.float64(1.11), 'y': np.float64(-0.02)}}\n",
      "\n",
      "0: 384x640 1 dog, 216.3ms\n",
      "Speed: 1.9ms preprocess, 216.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 86, 'id': 0, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(1.01), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 1 dog, 231.5ms\n",
      "Speed: 1.2ms preprocess, 231.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 87, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(1.01), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 2 dogs, 205.4ms\n",
      "Speed: 1.6ms preprocess, 205.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 88, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(0.99), 'y': np.float64(0.03)}}\n",
      "{'frame': 88, 'id': 1, 'class': 'dog', 'confidence': 0.47, 'rel_position': {'x': np.float64(-1.33), 'y': np.float64(-0.11)}}\n",
      "\n",
      "0: 384x640 2 dogs, 206.2ms\n",
      "Speed: 1.3ms preprocess, 206.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 89, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.92), 'y': np.float64(0.14)}}\n",
      "{'frame': 89, 'id': 1, 'class': 'dog', 'confidence': 0.79, 'rel_position': {'x': np.float64(-1.36), 'y': np.float64(0.02)}}\n",
      "\n",
      "0: 384x640 2 dogs, 215.0ms\n",
      "Speed: 1.7ms preprocess, 215.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 90, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.92), 'y': np.float64(0.14)}}\n",
      "{'frame': 90, 'id': 1, 'class': 'dog', 'confidence': 0.81, 'rel_position': {'x': np.float64(-1.36), 'y': np.float64(0.02)}}\n",
      "\n",
      "0: 384x640 2 dogs, 1 skis, 224.9ms\n",
      "Speed: 1.9ms preprocess, 224.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 91, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.19)}}\n",
      "{'frame': 91, 'id': 1, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(-1.34), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 2 dogs, 1 skis, 220.1ms\n",
      "Speed: 2.1ms preprocess, 220.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 92, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.2)}}\n",
      "{'frame': 92, 'id': 1, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(-1.34), 'y': np.float64(-0.01)}}\n",
      "\n",
      "0: 384x640 2 dogs, 233.2ms\n",
      "Speed: 1.5ms preprocess, 233.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 93, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.94), 'y': np.float64(0.14)}}\n",
      "{'frame': 93, 'id': 1, 'class': 'dog', 'confidence': 0.59, 'rel_position': {'x': np.float64(-1.37), 'y': np.float64(0.01)}}\n",
      "\n",
      "0: 384x640 2 dogs, 198.8ms\n",
      "Speed: 1.7ms preprocess, 198.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 94, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.94), 'y': np.float64(0.14)}}\n",
      "{'frame': 94, 'id': 1, 'class': 'dog', 'confidence': 0.66, 'rel_position': {'x': np.float64(-1.37), 'y': np.float64(0.01)}}\n",
      "\n",
      "0: 384x640 2 dogs, 1 sheep, 213.2ms\n",
      "Speed: 1.1ms preprocess, 213.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 95, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.91), 'y': np.float64(0.12)}}\n",
      "{'frame': 95, 'id': 1, 'class': 'dog', 'confidence': 0.6, 'rel_position': {'x': np.float64(-1.36), 'y': np.float64(-0.03)}}\n",
      "\n",
      "0: 384x640 2 dogs, 219.0ms\n",
      "Speed: 1.3ms preprocess, 219.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 96, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.86), 'y': np.float64(0.12)}}\n",
      "{'frame': 96, 'id': 1, 'class': 'dog', 'confidence': 0.82, 'rel_position': {'x': np.float64(-1.3), 'y': np.float64(-0.17)}}\n",
      "\n",
      "0: 384x640 2 dogs, 173.0ms\n",
      "Speed: 1.5ms preprocess, 173.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 97, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.86), 'y': np.float64(0.12)}}\n",
      "{'frame': 97, 'id': 1, 'class': 'dog', 'confidence': 0.83, 'rel_position': {'x': np.float64(-1.3), 'y': np.float64(-0.17)}}\n",
      "\n",
      "0: 384x640 3 dogs, 210.7ms\n",
      "Speed: 1.1ms preprocess, 210.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 98, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.79), 'y': np.float64(0.08)}}\n",
      "{'frame': 98, 'id': 1, 'class': 'dog', 'confidence': 0.76, 'rel_position': {'x': np.float64(-1.4), 'y': np.float64(-0.24)}}\n",
      "{'frame': 98, 'id': 2, 'class': 'dog', 'confidence': 0.3, 'rel_position': {'x': np.float64(-1.39), 'y': np.float64(-1.2)}}\n",
      "\n",
      "0: 384x640 3 dogs, 291.6ms\n",
      "Speed: 3.1ms preprocess, 291.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 99, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.79), 'y': np.float64(0.08)}}\n",
      "{'frame': 99, 'id': 1, 'class': 'dog', 'confidence': 0.7, 'rel_position': {'x': np.float64(-1.4), 'y': np.float64(-0.24)}}\n",
      "{'frame': 99, 'id': 2, 'class': 'dog', 'confidence': 0.26, 'rel_position': {'x': np.float64(-1.4), 'y': np.float64(-1.21)}}\n",
      "\n",
      "0: 384x640 1 dog, 271.2ms\n",
      "Speed: 2.7ms preprocess, 271.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 100, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.81), 'y': np.float64(0.11)}}\n",
      "\n",
      "0: 384x640 1 dog, 222.0ms\n",
      "Speed: 1.3ms preprocess, 222.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 101, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.85), 'y': np.float64(0.18)}}\n",
      "\n",
      "0: 384x640 1 dog, 215.0ms\n",
      "Speed: 1.6ms preprocess, 215.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 102, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.85), 'y': np.float64(0.18)}}\n",
      "\n",
      "0: 384x640 1 dog, 1 umbrella, 210.9ms\n",
      "Speed: 1.8ms preprocess, 210.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 103, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.84), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 1 dog, 1 umbrella, 229.4ms\n",
      "Speed: 3.1ms preprocess, 229.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 104, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.84), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 2 dogs, 209.7ms\n",
      "Speed: 1.4ms preprocess, 209.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 105, 'id': 0, 'class': 'dog', 'confidence': 0.97, 'rel_position': {'x': np.float64(0.84), 'y': np.float64(0.23)}}\n",
      "{'frame': 105, 'id': 1, 'class': 'dog', 'confidence': 0.46, 'rel_position': {'x': np.float64(-1.51), 'y': np.float64(-0.18)}}\n",
      "\n",
      "0: 384x640 3 dogs, 212.2ms\n",
      "Speed: 1.5ms preprocess, 212.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 106, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(0.83), 'y': np.float64(0.19)}}\n",
      "{'frame': 106, 'id': 1, 'class': 'dog', 'confidence': 0.37, 'rel_position': {'x': np.float64(-1.55), 'y': np.float64(-1.3)}}\n",
      "{'frame': 106, 'id': 2, 'class': 'dog', 'confidence': 0.26, 'rel_position': {'x': np.float64(-1.55), 'y': np.float64(-0.37)}}\n",
      "\n",
      "0: 384x640 2 dogs, 254.9ms\n",
      "Speed: 2.6ms preprocess, 254.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 107, 'id': 0, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(0.83), 'y': np.float64(0.19)}}\n",
      "{'frame': 107, 'id': 1, 'class': 'dog', 'confidence': 0.34, 'rel_position': {'x': np.float64(-1.55), 'y': np.float64(-1.3)}}\n",
      "\n",
      "0: 384x640 3 dogs, 231.5ms\n",
      "Speed: 2.4ms preprocess, 231.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 108, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.78), 'y': np.float64(0.21)}}\n",
      "{'frame': 108, 'id': 1, 'class': 'dog', 'confidence': 0.59, 'rel_position': {'x': np.float64(-1.56), 'y': np.float64(-0.28)}}\n",
      "{'frame': 108, 'id': 2, 'class': 'dog', 'confidence': 0.25, 'rel_position': {'x': np.float64(-1.55), 'y': np.float64(-1.29)}}\n",
      "\n",
      "0: 384x640 2 dogs, 200.4ms\n",
      "Speed: 1.1ms preprocess, 200.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 109, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.78), 'y': np.float64(0.21)}}\n",
      "{'frame': 109, 'id': 1, 'class': 'dog', 'confidence': 0.55, 'rel_position': {'x': np.float64(-1.56), 'y': np.float64(-0.28)}}\n",
      "\n",
      "0: 384x640 1 dog, 230.6ms\n",
      "Speed: 1.3ms preprocess, 230.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 110, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.67), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 1 dog, 234.5ms\n",
      "Speed: 1.9ms preprocess, 234.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 111, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.69), 'y': np.float64(0.21)}}\n",
      "\n",
      "0: 384x640 1 dog, 221.2ms\n",
      "Speed: 2.6ms preprocess, 221.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 112, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.69), 'y': np.float64(0.21)}}\n",
      "\n",
      "0: 384x640 3 dogs, 207.2ms\n",
      "Speed: 1.4ms preprocess, 207.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 113, 'id': 0, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(0.74), 'y': np.float64(0.3)}}\n",
      "{'frame': 113, 'id': 1, 'class': 'dog', 'confidence': 0.4, 'rel_position': {'x': np.float64(-1.68), 'y': np.float64(-0.87)}}\n",
      "{'frame': 113, 'id': 2, 'class': 'dog', 'confidence': 0.37, 'rel_position': {'x': np.float64(0.78), 'y': np.float64(-0.14)}}\n",
      "\n",
      "0: 384x640 3 dogs, 219.9ms\n",
      "Speed: 1.2ms preprocess, 219.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 114, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(0.74), 'y': np.float64(0.3)}}\n",
      "{'frame': 114, 'id': 1, 'class': 'dog', 'confidence': 0.35, 'rel_position': {'x': np.float64(0.78), 'y': np.float64(-0.15)}}\n",
      "{'frame': 114, 'id': 2, 'class': 'dog', 'confidence': 0.32, 'rel_position': {'x': np.float64(-1.68), 'y': np.float64(-0.88)}}\n",
      "\n",
      "0: 384x640 2 dogs, 221.9ms\n",
      "Speed: 1.9ms preprocess, 221.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 115, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.83), 'y': np.float64(0.32)}}\n",
      "{'frame': 115, 'id': 1, 'class': 'dog', 'confidence': 0.41, 'rel_position': {'x': np.float64(-1.68), 'y': np.float64(-0.74)}}\n",
      "\n",
      "0: 384x640 1 bus, 2 dogs, 218.9ms\n",
      "Speed: 1.4ms preprocess, 218.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 116, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.34)}}\n",
      "{'frame': 116, 'id': 1, 'class': 'dog', 'confidence': 0.55, 'rel_position': {'x': np.float64(-1.68), 'y': np.float64(-0.68)}}\n",
      "\n",
      "0: 384x640 1 bus, 2 dogs, 202.9ms\n",
      "Speed: 1.7ms preprocess, 202.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 117, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.34)}}\n",
      "{'frame': 117, 'id': 1, 'class': 'dog', 'confidence': 0.53, 'rel_position': {'x': np.float64(-1.68), 'y': np.float64(-0.68)}}\n",
      "\n",
      "0: 384x640 1 dog, 207.3ms\n",
      "Speed: 1.2ms preprocess, 207.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 118, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.93), 'y': np.float64(0.33)}}\n",
      "\n",
      "0: 384x640 1 dog, 218.2ms\n",
      "Speed: 2.0ms preprocess, 218.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 119, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.93), 'y': np.float64(0.32)}}\n",
      "\n",
      "0: 384x640 1 dog, 285.7ms\n",
      "Speed: 2.1ms preprocess, 285.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 120, 'id': 0, 'class': 'dog', 'confidence': 0.9, 'rel_position': {'x': np.float64(0.94), 'y': np.float64(0.32)}}\n",
      "\n",
      "0: 384x640 1 dog, 248.1ms\n",
      "Speed: 3.5ms preprocess, 248.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 121, 'id': 0, 'class': 'dog', 'confidence': 0.87, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.34)}}\n",
      "\n",
      "0: 384x640 1 dog, 211.1ms\n",
      "Speed: 1.9ms preprocess, 211.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 122, 'id': 0, 'class': 'dog', 'confidence': 0.88, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.34)}}\n",
      "\n",
      "0: 384x640 1 dog, 220.7ms\n",
      "Speed: 2.8ms preprocess, 220.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 123, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.92), 'y': np.float64(0.32)}}\n",
      "\n",
      "0: 384x640 1 dog, 229.9ms\n",
      "Speed: 4.3ms preprocess, 229.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 124, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.92), 'y': np.float64(0.32)}}\n",
      "\n",
      "0: 384x640 1 dog, 228.8ms\n",
      "Speed: 2.3ms preprocess, 228.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 125, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.89), 'y': np.float64(0.35)}}\n",
      "\n",
      "0: 384x640 1 dog, 213.2ms\n",
      "Speed: 1.9ms preprocess, 213.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 126, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.92), 'y': np.float64(0.36)}}\n",
      "\n",
      "0: 384x640 1 dog, 211.5ms\n",
      "Speed: 1.2ms preprocess, 211.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 127, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.92), 'y': np.float64(0.36)}}\n",
      "\n",
      "0: 384x640 1 dog, 224.4ms\n",
      "Speed: 1.2ms preprocess, 224.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 128, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.94), 'y': np.float64(0.41)}}\n",
      "\n",
      "0: 384x640 1 dog, 239.4ms\n",
      "Speed: 2.2ms preprocess, 239.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 129, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(0.94), 'y': np.float64(0.41)}}\n",
      "\n",
      "0: 384x640 1 dog, 215.5ms\n",
      "Speed: 1.2ms preprocess, 215.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 130, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.91), 'y': np.float64(0.51)}}\n",
      "\n",
      "0: 384x640 1 dog, 1 horse, 215.4ms\n",
      "Speed: 1.4ms preprocess, 215.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 131, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.52)}}\n",
      "\n",
      "0: 384x640 2 dogs, 1 horse, 223.8ms\n",
      "Speed: 2.1ms preprocess, 223.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 132, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.52)}}\n",
      "{'frame': 132, 'id': 2, 'class': 'dog', 'confidence': 0.36, 'rel_position': {'x': np.float64(-1.79), 'y': np.float64(0.29)}}\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 228.6ms\n",
      "Speed: 2.5ms preprocess, 228.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 133, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.89), 'y': np.float64(0.51)}}\n",
      "{'frame': 133, 'id': 1, 'class': 'person', 'confidence': 0.36, 'rel_position': {'x': np.float64(-1.79), 'y': np.float64(0.31)}}\n",
      "{'frame': 133, 'id': 2, 'class': 'person', 'confidence': 0.31, 'rel_position': {'x': np.float64(-1.73), 'y': np.float64(0.13)}}\n",
      "\n",
      "0: 384x640 2 persons, 1 dog, 230.6ms\n",
      "Speed: 1.8ms preprocess, 230.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 134, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(0.89), 'y': np.float64(0.51)}}\n",
      "{'frame': 134, 'id': 1, 'class': 'person', 'confidence': 0.39, 'rel_position': {'x': np.float64(-1.79), 'y': np.float64(0.31)}}\n",
      "{'frame': 134, 'id': 2, 'class': 'person', 'confidence': 0.29, 'rel_position': {'x': np.float64(-1.73), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 1 dog, 222.5ms\n",
      "Speed: 1.7ms preprocess, 222.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 135, 'id': 0, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.46)}}\n",
      "\n",
      "0: 384x640 1 bird, 2 dogs, 216.5ms\n",
      "Speed: 1.9ms preprocess, 216.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 136, 'id': 0, 'class': 'dog', 'confidence': 0.62, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.42)}}\n",
      "{'frame': 136, 'id': 1, 'class': 'dog', 'confidence': 0.5, 'rel_position': {'x': np.float64(-1.62), 'y': np.float64(-0.04)}}\n",
      "\n",
      "0: 384x640 1 bird, 2 dogs, 258.1ms\n",
      "Speed: 1.6ms preprocess, 258.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 137, 'id': 0, 'class': 'dog', 'confidence': 0.6, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.42)}}\n",
      "{'frame': 137, 'id': 1, 'class': 'dog', 'confidence': 0.55, 'rel_position': {'x': np.float64(-1.62), 'y': np.float64(-0.03)}}\n",
      "\n",
      "0: 384x640 1 person, 2 dogs, 1 cow, 232.3ms\n",
      "Speed: 2.3ms preprocess, 232.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 138, 'id': 0, 'class': 'dog', 'confidence': 0.69, 'rel_position': {'x': np.float64(0.86), 'y': np.float64(0.34)}}\n",
      "{'frame': 138, 'id': 2, 'class': 'person', 'confidence': 0.37, 'rel_position': {'x': np.float64(-1.56), 'y': np.float64(-0.05)}}\n",
      "{'frame': 138, 'id': 3, 'class': 'dog', 'confidence': 0.34, 'rel_position': {'x': np.float64(-1.55), 'y': np.float64(-0.05)}}\n",
      "\n",
      "0: 384x640 2 dogs, 1 cow, 212.9ms\n",
      "Speed: 1.6ms preprocess, 212.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 139, 'id': 0, 'class': 'dog', 'confidence': 0.68, 'rel_position': {'x': np.float64(-1.58), 'y': np.float64(0.18)}}\n",
      "{'frame': 139, 'id': 1, 'class': 'dog', 'confidence': 0.54, 'rel_position': {'x': np.float64(0.87), 'y': np.float64(0.41)}}\n",
      "\n",
      "0: 384x640 2 dogs, 1 cow, 216.3ms\n",
      "Speed: 1.4ms preprocess, 216.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 140, 'id': 0, 'class': 'dog', 'confidence': 0.69, 'rel_position': {'x': np.float64(-1.58), 'y': np.float64(0.18)}}\n",
      "{'frame': 140, 'id': 1, 'class': 'dog', 'confidence': 0.51, 'rel_position': {'x': np.float64(0.87), 'y': np.float64(0.41)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 226.4ms\n",
      "Speed: 2.7ms preprocess, 226.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 141, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.89), 'y': np.float64(0.23)}}\n",
      "{'frame': 141, 'id': 1, 'class': 'cat', 'confidence': 0.63, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.31)}}\n",
      "{'frame': 141, 'id': 2, 'class': 'dog', 'confidence': 0.44, 'rel_position': {'x': np.float64(0.88), 'y': np.float64(0.31)}}\n",
      "\n",
      "0: 384x640 2 dogs, 212.0ms\n",
      "Speed: 1.4ms preprocess, 212.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 142, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.88), 'y': np.float64(0.18)}}\n",
      "{'frame': 142, 'id': 1, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.29)}}\n",
      "\n",
      "0: 384x640 2 dogs, 219.8ms\n",
      "Speed: 2.0ms preprocess, 219.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 143, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.88), 'y': np.float64(0.18)}}\n",
      "{'frame': 143, 'id': 1, 'class': 'dog', 'confidence': 0.93, 'rel_position': {'x': np.float64(0.9), 'y': np.float64(0.29)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 210.5ms\n",
      "Speed: 1.8ms preprocess, 210.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 144, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.85), 'y': np.float64(0.11)}}\n",
      "{'frame': 144, 'id': 1, 'class': 'cat', 'confidence': 0.77, 'rel_position': {'x': np.float64(0.94), 'y': np.float64(0.25)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 236.0ms\n",
      "Speed: 1.5ms preprocess, 236.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 145, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.83), 'y': np.float64(0.08)}}\n",
      "{'frame': 145, 'id': 1, 'class': 'cat', 'confidence': 0.88, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 1 cat, 1 dog, 203.8ms\n",
      "Speed: 1.2ms preprocess, 203.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 146, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.83), 'y': np.float64(0.08)}}\n",
      "{'frame': 146, 'id': 1, 'class': 'cat', 'confidence': 0.88, 'rel_position': {'x': np.float64(0.96), 'y': np.float64(0.22)}}\n",
      "\n",
      "0: 384x640 2 dogs, 201.1ms\n",
      "Speed: 1.2ms preprocess, 201.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 147, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.81), 'y': np.float64(0.06)}}\n",
      "{'frame': 147, 'id': 1, 'class': 'dog', 'confidence': 0.72, 'rel_position': {'x': np.float64(0.99), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 2 dogs, 224.0ms\n",
      "Speed: 2.0ms preprocess, 224.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 148, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.81), 'y': np.float64(0.06)}}\n",
      "{'frame': 148, 'id': 1, 'class': 'dog', 'confidence': 0.75, 'rel_position': {'x': np.float64(0.99), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 2 dogs, 223.2ms\n",
      "Speed: 1.8ms preprocess, 223.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 149, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.78), 'y': np.float64(0.03)}}\n",
      "{'frame': 149, 'id': 1, 'class': 'dog', 'confidence': 0.7, 'rel_position': {'x': np.float64(1.03), 'y': np.float64(0.2)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 204.5ms\n",
      "Speed: 2.2ms preprocess, 204.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 150, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.76), 'y': np.float64(0.03)}}\n",
      "{'frame': 150, 'id': 1, 'class': 'dog', 'confidence': 0.68, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(0.18)}}\n",
      "{'frame': 150, 'id': 2, 'class': 'cat', 'confidence': 0.51, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(0.19)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 193.7ms\n",
      "Speed: 1.9ms preprocess, 193.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 151, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.76), 'y': np.float64(0.03)}}\n",
      "{'frame': 151, 'id': 1, 'class': 'dog', 'confidence': 0.72, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(0.18)}}\n",
      "{'frame': 151, 'id': 2, 'class': 'cat', 'confidence': 0.46, 'rel_position': {'x': np.float64(1.05), 'y': np.float64(0.19)}}\n",
      "\n",
      "0: 384x640 2 dogs, 224.0ms\n",
      "Speed: 1.5ms preprocess, 224.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 152, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.73), 'y': np.float64(0.07)}}\n",
      "{'frame': 152, 'id': 1, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(1.08), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 2 dogs, 221.9ms\n",
      "Speed: 1.3ms preprocess, 221.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 153, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.73), 'y': np.float64(0.07)}}\n",
      "{'frame': 153, 'id': 1, 'class': 'dog', 'confidence': 0.89, 'rel_position': {'x': np.float64(1.08), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 1 cat, 2 dogs, 209.0ms\n",
      "Speed: 1.1ms preprocess, 209.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 154, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.71), 'y': np.float64(0.06)}}\n",
      "{'frame': 154, 'id': 1, 'class': 'cat', 'confidence': 0.56, 'rel_position': {'x': np.float64(1.1), 'y': np.float64(0.09)}}\n",
      "{'frame': 154, 'id': 2, 'class': 'dog', 'confidence': 0.53, 'rel_position': {'x': np.float64(1.1), 'y': np.float64(0.09)}}\n",
      "\n",
      "0: 384x640 2 dogs, 210.2ms\n",
      "Speed: 1.2ms preprocess, 210.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 155, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.68), 'y': np.float64(0.06)}}\n",
      "{'frame': 155, 'id': 1, 'class': 'dog', 'confidence': 0.83, 'rel_position': {'x': np.float64(1.11), 'y': np.float64(0.06)}}\n",
      "\n",
      "0: 384x640 2 dogs, 223.2ms\n",
      "Speed: 1.2ms preprocess, 223.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 156, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.68), 'y': np.float64(0.06)}}\n",
      "{'frame': 156, 'id': 1, 'class': 'dog', 'confidence': 0.83, 'rel_position': {'x': np.float64(1.11), 'y': np.float64(0.07)}}\n",
      "\n",
      "0: 384x640 2 dogs, 217.9ms\n",
      "Speed: 2.0ms preprocess, 217.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 157, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.64), 'y': np.float64(0.09)}}\n",
      "{'frame': 157, 'id': 1, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.22), 'y': np.float64(0.1)}}\n",
      "\n",
      "0: 384x640 2 dogs, 215.9ms\n",
      "Speed: 2.1ms preprocess, 215.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 158, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.64), 'y': np.float64(0.09)}}\n",
      "{'frame': 158, 'id': 1, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(1.22), 'y': np.float64(0.1)}}\n",
      "\n",
      "0: 384x640 2 dogs, 216.0ms\n",
      "Speed: 1.7ms preprocess, 216.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 159, 'id': 0, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.6), 'y': np.float64(0.11)}}\n",
      "{'frame': 159, 'id': 1, 'class': 'dog', 'confidence': 0.92, 'rel_position': {'x': np.float64(1.2), 'y': np.float64(0.12)}}\n",
      "\n",
      "0: 384x640 2 dogs, 224.3ms\n",
      "Speed: 1.9ms preprocess, 224.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 160, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.2)}}\n",
      "{'frame': 160, 'id': 1, 'class': 'dog', 'confidence': 0.94, 'rel_position': {'x': np.float64(-0.52), 'y': np.float64(0.11)}}\n",
      "\n",
      "0: 384x640 2 dogs, 227.1ms\n",
      "Speed: 1.4ms preprocess, 227.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 161, 'id': 0, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.2)}}\n",
      "{'frame': 161, 'id': 1, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.52), 'y': np.float64(0.11)}}\n",
      "\n",
      "0: 384x640 2 dogs, 287.6ms\n",
      "Speed: 2.8ms preprocess, 287.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 162, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.3)}}\n",
      "{'frame': 162, 'id': 1, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.5), 'y': np.float64(0.11)}}\n",
      "\n",
      "0: 384x640 2 dogs, 220.5ms\n",
      "Speed: 1.7ms preprocess, 220.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 163, 'id': 0, 'class': 'dog', 'confidence': 0.96, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.3)}}\n",
      "{'frame': 163, 'id': 1, 'class': 'dog', 'confidence': 0.95, 'rel_position': {'x': np.float64(-0.5), 'y': np.float64(0.11)}}\n",
      "\n",
      "0: 384x640 2 dogs, 233.6ms\n",
      "Speed: 2.3ms preprocess, 233.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'frame': 164, 'id': 0, 'class': 'dog', 'confidence': 0.84, 'rel_position': {'x': np.float64(-0.38), 'y': np.float64(0.09)}}\n",
      "{'frame': 164, 'id': 1, 'class': 'dog', 'confidence': 0.79, 'rel_position': {'x': np.float64(1.15), 'y': np.float64(0.32)}}\n",
      "\n",
      "✅ Video processing complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     27\u001b[39m annotated_frame = frame.copy()\n\u001b[32m     28\u001b[39m output_data = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:185\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    157\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    158\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    159\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    160\u001b[39m     **kwargs: Any,\n\u001b[32m    161\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    164\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:555\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:227\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:646\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:138\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:156\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:179\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    180\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:318\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:318\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m y.extend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:495\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    494\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28mself\u001b[39m.cv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:92\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     83\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Real\\ObjectDetection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 7: Load video file and process\n",
    "video_path = video_path_widget.value\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"❌ File not found: {video_path}\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ Unable to open video.\")\n",
    "    else:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"❌ Unable to read video.\")\n",
    "        else:\n",
    "            h, w = frame.shape[:2]\n",
    "            frame_center = np.array([w / 2, h / 2])\n",
    "            print(\"🎥 Processing video...\")\n",
    "\n",
    "            # Create a named window for displaying images\n",
    "            cv2.namedWindow('Annotated Frame', cv2.WINDOW_NORMAL)\n",
    "\n",
    "            try:\n",
    "                frame_count = 0\n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        print(\"✅ End of video stream.\")\n",
    "                        break\n",
    "\n",
    "                    results = model(frame)[0]\n",
    "                    annotated_frame = frame.copy()\n",
    "                    output_data = []\n",
    "\n",
    "                    for i, det in enumerate(results.boxes):\n",
    "                        cls_id = int(det.cls[0])\n",
    "                        cls_name = COCO_CLASSES[cls_id]\n",
    "\n",
    "                        if cls_name not in class_selector.value:\n",
    "                            continue\n",
    "\n",
    "                        conf = float(det.conf[0])\n",
    "                        x1, y1, x2, y2 = map(int, det.xyxy[0])\n",
    "                        box_center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
    "                        rel_x, rel_y = get_relative_position(box_center, frame_center)\n",
    "\n",
    "                        label = f\"{cls_name} {conf:.2f} [{rel_x}, {rel_y}]\"\n",
    "                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        cv2.putText(annotated_frame, label, (x1, y1 - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "                        output_data.append({\n",
    "                            \"frame\": frame_count,\n",
    "                            \"id\": i,\n",
    "                            \"class\": cls_name,\n",
    "                            \"confidence\": round(conf, 2),\n",
    "                            \"rel_position\": {\"x\": rel_x, \"y\": rel_y}\n",
    "                        })\n",
    "\n",
    "                    for obj in output_data:\n",
    "                        print(obj)\n",
    "\n",
    "                    # Display the annotated frame using OpenCV in the named window\n",
    "                    cv2.imshow('Annotated Frame', annotated_frame)\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "                        break\n",
    "\n",
    "                    frame_count += 1\n",
    "\n",
    "            finally:\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()  # Close the OpenCV window\n",
    "                print(\"✅ Video processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
